{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning-based Image Captioning with R-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from utils.image_utils import image_from_url\n",
    "\n",
    "from torchsummary import summary\n",
    "from keybert import KeyBERT\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Working on: \", device)\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "max_seq_len = 17\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "import metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MS-COCO data\n",
    "Using the Microsoft COCO dataset for captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "data[\"train_captions_lens\"] = np.zeros(data[\"train_captions\"].shape[0])\n",
    "data[\"val_captions_lens\"] = np.zeros(data[\"val_captions\"].shape[0])\n",
    "for i in range(data[\"train_captions\"].shape[0]):\n",
    "    data[\"train_captions_lens\"][i] = np.nonzero(data[\"train_captions\"][i] == 2)[0][0] + 1\n",
    "for i in range(data[\"val_captions\"].shape[0]):\n",
    "    data[\"val_captions_lens\"][i] = np.nonzero(data[\"val_captions\"][i] == 2)[0][0] + 1\n",
    "\n",
    "\n",
    "# Print out all the keys and values from the data dictionary\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = load_coco_data(max_train=50000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        \n",
    "        vocab_size = len(word_to_idx)\n",
    "\n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        \n",
    "        self.cnn2linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(wordvec_dim, hidden_dim, batch_first=True)\n",
    "        self.linear2vocab = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        hidden_init = self.cnn2linear(features)\n",
    "        cell_init = torch.zeros_like(hidden_init)\n",
    "        output, _ = self.lstm(input_captions, (hidden_init, cell_init))\n",
    "        output = self.linear2vocab(output)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardNetworkRNN(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(RewardNetworkRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.hidden_cell = torch.zeros(1, 1, self.hidden_dim).to(device)\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        self.gru = nn.GRU(wordvec_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        output, self.hidden_cell = self.gru(input_captions.view(len(input_captions) ,1, -1), self.hidden_cell)\n",
    "        return output\n",
    "    \n",
    "class RewardNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx):\n",
    "        super(RewardNetwork, self).__init__()\n",
    "        self.rewrnn = RewardNetworkRNN(word_to_idx)\n",
    "        self.visual_embed = nn.Linear(512, 512)\n",
    "        self.semantic_embed = nn.Linear(512, 512)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        for t in range(captions.shape[1]):\n",
    "            rrnn = self.rewrnn(captions[:, t])\n",
    "        rrnn = rrnn.squeeze(0).squeeze(1)\n",
    "        se = self.semantic_embed(rrnn)\n",
    "        ve = self.visual_embed(features)\n",
    "        return ve, se"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetworkRNN(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(ValueNetworkRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.hidden_cell = (torch.zeros(1, 1, self.hidden_dim).to(device), torch.zeros(1, 1, self.hidden_dim).to(device))\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        self.lstm = nn.LSTM(wordvec_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        output, self.hidden_cell = self.lstm(input_captions.view(len(input_captions) ,1, -1), self.hidden_cell)\n",
    "        return output\n",
    "    \n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.valrnn = ValueNetworkRNN(word_to_idx)\n",
    "        self.linear1 = nn.Linear(1024, 512)\n",
    "        self.linear2 = nn.Linear(512, 1)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        for t in range(captions.shape[1]):\n",
    "            vrnn = self.valrnn(captions[:, t])\n",
    "        vrnn = vrnn.squeeze(0).squeeze(1)\n",
    "        state = torch.cat((features, vrnn), dim=1)\n",
    "        output = self.linear1(state)\n",
    "        output = self.linear2(output)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet.load_state_dict(torch.load('policyNetwork.pt'))\n",
    "policyNet.train(mode=False)\n",
    "\n",
    "valueNet = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "valueNet.load_state_dict(torch.load('valueNetwork.pt'))\n",
    "valueNet.train(mode=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Caption Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCaptions(features, captions, model):\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    gen_caps = torch.tensor(captions[:, 0:1], device=device).long()\n",
    "    for t in range(max_seq_len-1):\n",
    "        output = model(features, gen_caps)\n",
    "        gen_caps = torch.cat((gen_caps, output[:, -1:, :].argmax(axis=2)), axis=1)\n",
    "    return gen_caps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search Caption Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCaptionsWithBeamSearch(features, captions, model, beamSize=5):\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    gen_caps = torch.tensor(captions[:, 0:1], device=device).long()\n",
    "    candidates = [(gen_caps, 0)]\n",
    "    for t in range(max_seq_len-1):\n",
    "        next_candidates = []\n",
    "        for c in range(len(candidates)):\n",
    "            output = model(features, candidates[c][0])\n",
    "            probs, words = torch.topk(output[:, -1:, :], beamSize)\n",
    "            for i in range(beamSize):\n",
    "                cap = torch.cat((candidates[c][0], words[:, :, i]), axis=1)\n",
    "                score = candidates[c][1] - torch.log(probs[0, 0, i]).item()\n",
    "                next_candidates.append((cap, score))\n",
    "        ordered_candidates = sorted(next_candidates, key=lambda tup:tup[1])\n",
    "        candidates = ordered_candidates[:beamSize]\n",
    "    return candidates "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lookahead Inference with Policy and Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCaptionsWithBeamSearchValueScoring(features, captions, model, beamSize=5):\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    gen_caps = torch.tensor(captions[:, 0:1], device=device).long()\n",
    "    candidates = [(gen_caps, 0)]\n",
    "    for t in range(max_seq_len-1):\n",
    "        next_candidates = []\n",
    "        for c in range(len(candidates)):\n",
    "            output = model(features, candidates[c][0])\n",
    "            probs, words = torch.topk(output[:, -1:, :], beamSize)\n",
    "            for i in range(beamSize):\n",
    "                cap = torch.cat((candidates[c][0], words[:, :, i]), axis=1)\n",
    "                value = valueNet(features.squeeze(0), cap).detach()\n",
    "                score = candidates[c][1] - 0.6*value.item() -0.4*torch.log(probs[0, 0, i]).item()\n",
    "                next_candidates.append((cap, score))\n",
    "        ordered_candidates = sorted(next_candidates, key=lambda tup:tup[1])\n",
    "        candidates = ordered_candidates[:beamSize]\n",
    "    return candidates   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_m = KeyBERT()\n",
    "with torch.no_grad():\n",
    "    max_seq_len = 17\n",
    "    captions, features, urls = sample_coco_minibatch(small_data, batch_size=100, split='val')\n",
    "    print(features)\n",
    "    for i in range(10):\n",
    "        gen_caps = []\n",
    "        gen_caps.append(GenerateCaptions(features[i:i+1], captions[i:i+1], policyNet)[0])\n",
    "        gen_caps.append(GenerateCaptionsWithBeamSearch(features[i:i+1], captions[i:i+1], policyNet)[0][0][0])\n",
    "        gen_caps.append(GenerateCaptionsWithBeamSearchValueScoring(features[i:i+1], captions[i:i+1], policyNet)[0][0][0])\n",
    "        decoded_tru_caps = decode_captions(captions[i], data[\"idx_to_word\"])\n",
    "\n",
    "#         f = open(\"truth3.txt\", \"a\")\n",
    "#         f.write(decoded_tru_caps + \"\\n\")\n",
    "#         f.close()\n",
    "        \n",
    "#         decoded_gen_caps = decode_captions(gen_caps[0], data[\"idx_to_word\"])\n",
    "#         f = open(\"greedy3.txt\", \"a\")\n",
    "#         f.write(decoded_gen_caps + \"\\n\")\n",
    "#         f.close()\n",
    "        \n",
    "#         decoded_gen_caps = decode_captions(gen_caps[1], data[\"idx_to_word\"])\n",
    "#         f = open(\"beam3.txt\", \"a\")\n",
    "#         f.write(decoded_gen_caps + \"\\n\")\n",
    "#         f.close()\n",
    "        \n",
    "#         decoded_gen_caps = decode_captions(gen_caps[2], data[\"idx_to_word\"])\n",
    "#         f = open(\"policyvalue3.txt\", \"a\")\n",
    "#         f.write(decoded_gen_caps + \"\\n\")\n",
    "#         f.close()\n",
    "        try:\n",
    "            plt.imshow(image_from_url(urls[i]))\n",
    "            plt.show()\n",
    "        except:\n",
    "            pass\n",
    "        keyword = k_m.extract_keywords(decode_captions(gen_caps[2], data[\"idx_to_word\"]).replace(\"<START>\",\"\").replace(\"<END>\",\"\"))\n",
    "        print(urls[i])\n",
    "        print(decode_captions(gen_caps[2], data[\"idx_to_word\"]))\n",
    "        print(keyword)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLEU_score(gt_caption, sample_caption, w):\n",
    "    \"\"\"\n",
    "    gt_caption: string, ground-truth caption\n",
    "    sample_caption: string, your model's predicted caption\n",
    "    Returns unigram BLEU score.\n",
    "    \"\"\"\n",
    "    reference = [x for x in gt_caption.split(' ') \n",
    "                 if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "    hypothesis = [x for x in sample_caption.split(' ') \n",
    "                  if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = [w])\n",
    "    return BLEUscore\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"\n",
    "    model: CaptioningRNN model\n",
    "    Prints unigram BLEU score averaged over 1000 training and val examples.\n",
    "    \"\"\"\n",
    "    BLEUscores = {}\n",
    "    for split in ['train', 'val']:\n",
    "        minibatch = sample_coco_minibatch(data, split=split, batch_size=1000)\n",
    "        gt_captions, features, urls = minibatch\n",
    "        gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
    "\n",
    "        sample_captions = model.sample(features)\n",
    "        sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n",
    "\n",
    "        total_score = 0.0\n",
    "        for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n",
    "            total_score += BLEU_score(gt_caption, sample_caption)\n",
    "\n",
    "        BLEUscores[split] = total_score / len(sample_captions)\n",
    "\n",
    "    for split in BLEUscores:\n",
    "        print('Average BLEU score for %s: %f' % (split, BLEUscores[split]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps0 = []\n",
    "caps1 = []\n",
    "caps2 = []\n",
    "caps3 = []\n",
    "f = open(\"results/truth3.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps0.append(x)\n",
    "f = open(\"results/greedy3.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps1.append(x)\n",
    "f = open(\"results/beam3.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps2.append(x)\n",
    "f = open(\"results/policyvalue3.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps3.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1, b2, b3 = 0, 0, 0\n",
    "for w in range(1, 5):\n",
    "    for i in range(len(caps0)):\n",
    "        b1 += BLEU_score(caps0[i], caps1[i], w)\n",
    "        b2 += BLEU_score(caps0[i], caps2[i], w)\n",
    "        b3 += BLEU_score(caps0[i], caps3[i], w)\n",
    "    b1 /= len(caps0)\n",
    "    b2 /= len(caps0)\n",
    "    b3 /= len(caps0)\n",
    "    print(\"Greedy BLEU-\" + str(w), \":\", b1)\n",
    "    print(\"Beam BLEU-\" + str(w), \":\", b2)\n",
    "    print(\"Agent BLEU-\" + str(w), \":\", b3)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref, hypo = metrics.load_textfiles(caps0, caps3)\n",
    "print(metrics.score(ref, hypo))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = True\n",
    "\n",
    "policyNetwork = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(policyNetwork.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "if pretrained:\n",
    "    policyNetwork.load_state_dict(torch.load('models/policyNetwork.pt'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "bestLoss = 50\n",
    "#0.006700546946376562\n",
    "\n",
    "for epoch in range(250000, 251000):\n",
    "    print(epoch)\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    captions_in = torch.tensor(captions[:, :-1], device=device).long()\n",
    "    captions_ou = torch.tensor(captions[:, 1:], device=device).long()\n",
    "    output = policyNetwork(features, captions_in)\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        caplen = np.nonzero(captions[i] == 2)[0][0] + 1\n",
    "        loss += (caplen/batch_size)*criterion(output[i][:caplen], captions_ou[i][:caplen])\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(policyNetwork.state_dict(), \"policyNetwork.pt\")\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Reward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardNetwork = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "optimizer = optim.Adam(rewardNetwork.parameters(), lr=0.001)\n",
    "\n",
    "# https://cs230-stanford.github.io/pytorch-nlp.html#writing-a-custom-loss-function\n",
    "def VisualSemanticEmbeddingLoss(visuals, semantics):\n",
    "    beta = 0.2\n",
    "    N, D = visuals.shape\n",
    "    \n",
    "    visloss = torch.mm(visuals, semantics.t())\n",
    "    visloss = visloss - torch.diag(visloss).unsqueeze(1)\n",
    "    visloss = visloss + (beta/N)*(torch.ones((N, N)).to(device) - torch.eye(N).to(device))\n",
    "    visloss = F.relu(visloss)\n",
    "    visloss = torch.sum(visloss)/N\n",
    "    \n",
    "    semloss = torch.mm(semantics, visuals.t())\n",
    "    semloss = semloss - torch.diag(semloss).unsqueeze(1)\n",
    "    semloss = semloss + (beta/N)*(torch.ones((N, N)).to(device) - torch.eye(N).to(device))\n",
    "    semloss = F.relu(semloss)\n",
    "    semloss = torch.sum(semloss)/N\n",
    "    \n",
    "    return visloss + semloss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "bestLoss = 10000\n",
    "\n",
    "for epoch in range(1000):\n",
    "    print(epoch)\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float()\n",
    "    captions = torch.tensor(captions, device=device).long()\n",
    "    ve, se = rewardNetwork(features, captions)\n",
    "    loss = VisualSemanticEmbeddingLoss(ve, se)\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(rewardNetwork.state_dict(), \"rewardNetwork.pt\")\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    rewardNetwork.rewrnn.hidden_cell.detach_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRewards(features, captions, model):\n",
    "    visEmbeds, semEmbeds = model(features, captions)\n",
    "    visEmbeds = F.normalize(visEmbeds, p=2, dim=1) \n",
    "    semEmbeds = F.normalize(semEmbeds, p=2, dim=1) \n",
    "    rewards = torch.sum(visEmbeds*semEmbeds, axis=1).unsqueeze(1)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardNet = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "rewardNet.load_state_dict(torch.load('rewardNetwork.pt'))\n",
    "for param in rewardNet.parameters():\n",
    "    param.require_grad = False\n",
    "print(rewardNet)\n",
    "\n",
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet.load_state_dict(torch.load('policyNetwork.pt'))\n",
    "for param in policyNet.parameters():\n",
    "    param.require_grad = False\n",
    "print(policyNet)\n",
    "\n",
    "valueNetwork = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(valueNetwork.parameters(), lr=0.0001)\n",
    "valueNetwork.train(mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "bestLoss = 10000\n",
    "max_seq_len = 17\n",
    "\n",
    "for epoch in range(1000):\n",
    "    print(epoch)\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float()\n",
    "    \n",
    "    # Generate captions using the policy network\n",
    "    captions = GenerateCaptions(features, captions, policyNet)\n",
    "    \n",
    "    # Compute the reward of the generated caption using reward network\n",
    "    rewards = GetRewards(features, captions, rewardNet)\n",
    "    \n",
    "    # Compute the value of a random state in the generation process\n",
    "#     print(features.shape, captions[:, :random.randint(1, 17)].shape)\n",
    "    values = valueNetwork(features, captions[:, :random.randint(1, 17)])\n",
    "    \n",
    "    # Compute the loss for the value and the reward\n",
    "    loss = criterion(values, rewards)\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(valueNetwork.state_dict(), \"valueNetwork.pt\")\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    valueNetwork.valrnn.hidden_cell[0].detach_()\n",
    "    valueNetwork.valrnn.hidden_cell[1].detach_()\n",
    "    rewardNet.rewrnn.hidden_cell.detach_()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "Advantage Actor Critic Model for Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvantageActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, valueNet, policyNet):\n",
    "        super(AdvantageActorCriticNetwork, self).__init__()\n",
    "\n",
    "        self.valueNet = valueNet #RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "        self.policyNet = policyNet #PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # Get value from value network\n",
    "        values = self.valueNet(features, captions)\n",
    "        # Get action probabilities from policy network\n",
    "        probs = self.policyNet(features.unsqueeze(0), captions)[:, -1:, :]        \n",
    "        return values, probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardNet = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "valueNet = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "\n",
    "rewardNet.load_state_dict(torch.load('rewardNetwork.pt'))\n",
    "policyNet.load_state_dict(torch.load('policyNetwork.pt'))\n",
    "valueNet.load_state_dict(torch.load('valueNetwork.pt'))\n",
    "\n",
    "a2cNetwork = AdvantageActorCriticNetwork(valueNet, policyNet)\n",
    "optimizer = optim.Adam(a2cNetwork.parameters(), lr=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curriculum = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "episodes = 20\n",
    "\n",
    "small_data = load_coco_data(max_train=50000)\n",
    "\n",
    "for level in curriculum:\n",
    "    \n",
    "    for epoch in range(100):        \n",
    "        episodicAvgLoss = 0\n",
    "        \n",
    "        captions, features, _ = sample_coco_minibatch(small_data, batch_size=episodes, split='train')\n",
    "        features = torch.tensor(features, device=device).float()\n",
    "        captions = torch.tensor(captions, device=device).long()\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            log_probs = []\n",
    "            values = []\n",
    "            rewards = []\n",
    "            caplen = np.nonzero(captions[episode] == 2)[0][0] + 1\n",
    "            \n",
    "            if (caplen - level > 1):\n",
    "                captions_in = captions[episode:episode+1, :caplen-level]\n",
    "                features_in = features[episode:episode+1]\n",
    "\n",
    "                for step in range(level):\n",
    "                    value, probs = a2cNetwork(features_in, captions_in)\n",
    "                    probs = F.softmax(probs, dim=2)\n",
    "                    \n",
    "                    dist = probs.cpu().detach().numpy()[0,0]\n",
    "                    action = np.random.choice(probs.shape[-1], p=dist)\n",
    "                    \n",
    "                    gen_cap = torch.from_numpy(np.array([action])).unsqueeze(0).to(device)\n",
    "                    captions_in = torch.cat((captions_in, gen_cap), axis=1)\n",
    "                    \n",
    "                    log_prob = torch.log(probs[0, 0, action])\n",
    "                    \n",
    "                    reward = GetRewards(features_in, captions_in, rewardNet)\n",
    "                    reward = reward.cpu().detach().numpy()[0, 0]\n",
    "                    \n",
    "                    rewards.append(reward)\n",
    "                    values.append(value)\n",
    "                    log_probs.append(log_prob)\n",
    "                    \n",
    "            values = torch.FloatTensor(values).to(device)\n",
    "            rewards = torch.FloatTensor(rewards).to(device)\n",
    "            log_probs = torch.stack(log_probs).to(device)\n",
    "            \n",
    "            advantage = values - rewards \n",
    "            actorLoss = (-log_probs * advantage).mean()\n",
    "            criticLoss = 0.5 * advantage.pow(2).mean()\n",
    "            \n",
    "            loss = actorLoss + criticLoss\n",
    "            episodicAvgLoss += loss.item()/episodes\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(epoch, \":\", episodicAvgLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "5375c6fca34c5f00efc2a1fb12328b4af153b74c7751f4a701371fb63dcb16d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
