{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning-based Image Captioning with R-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on:  cpu\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from utils.image_utils import image_from_url\n",
    "\n",
    "from torchsummary import summary\n",
    "from keybert import KeyBERT\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Working on: \", device)\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "max_seq_len = 17\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MS-COCO data\n",
    "Using the Microsoft COCO dataset for captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
      "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
      "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
      "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
      "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
      "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
      "idx_to_word <class 'list'> 1004\n",
      "word_to_idx <class 'dict'> 1004\n",
      "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
      "val_urls <class 'numpy.ndarray'> (40504,) <U63\n",
      "train_captions_lens <class 'numpy.ndarray'> (400135,) float64\n",
      "val_captions_lens <class 'numpy.ndarray'> (195954,) float64\n"
     ]
    }
   ],
   "source": [
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "data[\"train_captions_lens\"] = np.zeros(data[\"train_captions\"].shape[0])\n",
    "data[\"val_captions_lens\"] = np.zeros(data[\"val_captions\"].shape[0])\n",
    "for i in range(data[\"train_captions\"].shape[0]):\n",
    "    data[\"train_captions_lens\"][i] = np.nonzero(data[\"train_captions\"][i] == 2)[0][0] + 1\n",
    "for i in range(data[\"val_captions\"].shape[0]):\n",
    "    data[\"val_captions_lens\"][i] = np.nonzero(data[\"val_captions\"][i] == 2)[0][0] + 1\n",
    "\n",
    "\n",
    "# Print out all the keys and values from the data dictionary\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = load_coco_data(max_train=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        \n",
    "        vocab_size = len(word_to_idx)\n",
    "\n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        \n",
    "        self.cnn2linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(wordvec_dim, hidden_dim, batch_first=True)\n",
    "        self.linear2vocab = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        hidden_init = self.cnn2linear(features)\n",
    "        cell_init = torch.zeros_like(hidden_init)\n",
    "        output, _ = self.lstm(input_captions, (hidden_init, cell_init))\n",
    "        output = self.linear2vocab(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardNetworkRNN(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(RewardNetworkRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.hidden_cell = torch.zeros(1, 1, self.hidden_dim).to(device)\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        self.gru = nn.GRU(wordvec_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        output, self.hidden_cell = self.gru(input_captions.view(len(input_captions) ,1, -1), self.hidden_cell)\n",
    "        return output\n",
    "    \n",
    "class RewardNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx):\n",
    "        super(RewardNetwork, self).__init__()\n",
    "        self.rewrnn = RewardNetworkRNN(word_to_idx)\n",
    "        self.visual_embed = nn.Linear(512, 512)\n",
    "        self.semantic_embed = nn.Linear(512, 512)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        for t in range(captions.shape[1]):\n",
    "            rrnn = self.rewrnn(captions[:, t])\n",
    "        rrnn = rrnn.squeeze(0).squeeze(1)\n",
    "        se = self.semantic_embed(rrnn)\n",
    "        ve = self.visual_embed(features)\n",
    "        return ve, se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetworkRNN(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(ValueNetworkRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.hidden_cell = (torch.zeros(1, 1, self.hidden_dim).to(device), torch.zeros(1, 1, self.hidden_dim).to(device))\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        self.lstm = nn.LSTM(wordvec_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        output, self.hidden_cell = self.lstm(input_captions.view(len(input_captions) ,1, -1), self.hidden_cell)\n",
    "        return output\n",
    "    \n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.valrnn = ValueNetworkRNN(word_to_idx)\n",
    "        self.linear1 = nn.Linear(1024, 512)\n",
    "        self.linear2 = nn.Linear(512, 1)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        for t in range(captions.shape[1]):\n",
    "            vrnn = self.valrnn(captions[:, t])\n",
    "        vrnn = vrnn.squeeze(0).squeeze(1)\n",
    "        state = torch.cat((features, vrnn), dim=1)\n",
    "        output = self.linear1(state)\n",
    "        output = self.linear2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueNetwork(\n",
       "  (valrnn): ValueNetworkRNN(\n",
       "    (caption_embedding): Embedding(1004, 512)\n",
       "    (lstm): LSTM(512, 512)\n",
       "  )\n",
       "  (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet.load_state_dict(torch.load('policyNetwork.pt'))\n",
    "policyNet.train(mode=False)\n",
    "\n",
    "valueNet = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "valueNet.load_state_dict(torch.load('valueNetwork.pt'))\n",
    "valueNet.train(mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Caption Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCaptions(features, captions, model):\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    gen_caps = torch.tensor(captions[:, 0:1], device=device).long()\n",
    "    for t in range(max_seq_len-1):\n",
    "        output = model(features, gen_caps)\n",
    "        gen_caps = torch.cat((gen_caps, output[:, -1:, :].argmax(axis=2)), axis=1)\n",
    "    return gen_caps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search Caption Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCaptionsWithBeamSearch(features, captions, model, beamSize=5):\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    gen_caps = torch.tensor(captions[:, 0:1], device=device).long()\n",
    "    candidates = [(gen_caps, 0)]\n",
    "    for t in range(max_seq_len-1):\n",
    "        next_candidates = []\n",
    "        for c in range(len(candidates)):\n",
    "            output = model(features, candidates[c][0])\n",
    "            probs, words = torch.topk(output[:, -1:, :], beamSize)\n",
    "            for i in range(beamSize):\n",
    "                cap = torch.cat((candidates[c][0], words[:, :, i]), axis=1)\n",
    "                score = candidates[c][1] - torch.log(probs[0, 0, i]).item()\n",
    "                next_candidates.append((cap, score))\n",
    "        ordered_candidates = sorted(next_candidates, key=lambda tup:tup[1])\n",
    "        candidates = ordered_candidates[:beamSize]\n",
    "    return candidates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lookahead Inference with Policy and Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCaptionsWithBeamSearchValueScoring(features, captions, model, beamSize=5):\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    gen_caps = torch.tensor(captions[:, 0:1], device=device).long()\n",
    "    candidates = [(gen_caps, 0)]\n",
    "    for t in range(max_seq_len-1):\n",
    "        next_candidates = []\n",
    "        for c in range(len(candidates)):\n",
    "            output = model(features, candidates[c][0])\n",
    "            probs, words = torch.topk(output[:, -1:, :], beamSize)\n",
    "            for i in range(beamSize):\n",
    "                cap = torch.cat((candidates[c][0], words[:, :, i]), axis=1)\n",
    "                value = valueNet(features.squeeze(0), cap).detach()\n",
    "                score = candidates[c][1] - 0.6*value.item() -0.4*torch.log(probs[0, 0, i]).item()\n",
    "                next_candidates.append((cap, score))\n",
    "        ordered_candidates = sorted(next_candidates, key=lambda tup:tup[1])\n",
    "        candidates = ordered_candidates[:beamSize]\n",
    "    return candidates   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.2303767   -4.806963     8.533777   ...   0.28401917  -0.04026076\n",
      "   -0.25601593]\n",
      " [ 15.976098    -7.3382072   -7.347663   ...  -0.33290958  -0.96845496\n",
      "    1.6827646 ]\n",
      " [-22.686117    16.412437     0.4373119  ...  -0.89508474  -0.11228228\n",
      "   -1.2711093 ]\n",
      " ...\n",
      " [ 31.565493    11.678165    20.492212   ...  -0.31051296  -0.7093524\n",
      "   -0.58677226]\n",
      " [ 17.475239    18.201956   -17.278948   ...   0.8789816   -0.21191888\n",
      "   -1.2044929 ]\n",
      " [ -3.914217    -7.0956974  -12.596421   ...  -1.0031083    0.36291438\n",
      "   -2.407761  ]]\n",
      "http://farm1.staticflickr.com/22/25535574_f2347f2ff7_z.jpg\n",
      "<START> a man and a woman in a room with a <UNK> <UNK> <END>\n",
      "[('unk', 0.314), ('room', 0.2973), ('woman', 0.2703), ('man', 0.1953)]\n",
      "URL Error:  Gone http://farm4.staticflickr.com/3662/3639047570_1c0cc2b782_z.jpg\n",
      "http://farm4.staticflickr.com/3662/3639047570_1c0cc2b782_z.jpg\n",
      "<START> a street sign on top of a city street <END>\n",
      "[('street', 0.5512), ('city', 0.49), ('sign', 0.4394)]\n",
      "http://farm5.staticflickr.com/4027/4331270666_004e3f4349_z.jpg\n",
      "<START> a group of people sitting on top of a table <END>\n",
      "[('group', 0.5138), ('people', 0.4347), ('table', 0.3171), ('sitting', 0.2518)]\n",
      "http://farm5.staticflickr.com/4044/4656976729_51ace95313_z.jpg\n",
      "<START> a group of people standing in the water <END>\n",
      "[('water', 0.5117), ('group', 0.4665), ('people', 0.3396), ('standing', 0.2975)]\n",
      "http://farm4.staticflickr.com/3023/2581605114_9e24ec7da6_z.jpg\n",
      "<START> a man with a <UNK> <UNK> on a table <END>\n",
      "[('unk', 0.4577), ('man', 0.4379), ('table', 0.3685)]\n",
      "http://farm5.staticflickr.com/4115/4859291760_af67f36402_z.jpg\n",
      "<START> a cat is sitting on top of a table <END>\n",
      "[('cat', 0.629), ('table', 0.4034), ('sitting', 0.2344)]\n",
      "http://farm3.staticflickr.com/2539/4005478882_a889106411_z.jpg\n",
      "<START> a group of people standing in a field <END>\n",
      "[('field', 0.5115), ('group', 0.3883), ('standing', 0.3484), ('people', 0.339)]\n",
      "http://farm3.staticflickr.com/2867/9116803533_a937b951c4_z.jpg\n",
      "<START> a man in front of a <UNK> <UNK> in a <UNK> <UNK> <END>\n",
      "[('unk', 0.5325), ('man', 0.4051)]\n",
      "http://farm5.staticflickr.com/4092/5016273480_7e16210e69_z.jpg\n",
      "<START> a man sitting on top of a plate <END>\n",
      "[('plate', 0.5178), ('man', 0.4506), ('sitting', 0.394)]\n",
      "http://farm5.staticflickr.com/4118/5446124983_837a74a915_z.jpg\n",
      "<START> a man is sitting on top of a table <END>\n",
      "[('man', 0.4392), ('sitting', 0.3516), ('table', 0.3104)]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAHWCAYAAACv91olAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAARVElEQVR4nO3dX4il913H8c/X3Qa0/mlpVtEki1FS41400o6xiH9Si5rkJhR6kVRaDIUl2IiXDV7oRW/0QhBp6rKUULwxFza0qcQGQdoKNZoNtGnTkrKmmKwpJLGlQgXDtl8vZtRxOpt5dnLOfp3T1wsOzPOc35z58mPZ9z5nZp6t7g4AcGV93/QAAPC9SIABYIAAA8AAAQaAAQIMAAMEGAAGHBjgqnqgql6oqi9e4vmqqj+rqvNV9WRVvXn1YwLAZllyBfyRJLe+wvO3Jblh53E6yZ+/+rEAYLMdGODu/kySr7/CkjuS/EVveyzJ66rqx1c1IABsolV8D/iaJM/tOr6wcw4AuITjK3iN2ufcvve3rKrT2X6bOq997WvfcuONN67gywPAjCeeeOKl7j5xmM9dRYAvJLlu1/G1SZ7fb2F3n01yNkm2trb63LlzK/jyADCjqv7lsJ+7iregH07ynp2fhn5rkm9299dW8LoAsLEOvAKuqr9MckuSq6vqQpI/TPKaJOnuM0keSXJ7kvNJ/iPJ3esaFgA2xYEB7u67Dni+k7xvZRMBwPcAd8ICgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMGBRgKvq1qp6uqrOV9V9+zz/I1X1iar6fFU9VVV3r35UANgcBwa4qo4luT/JbUlOJbmrqk7tWfa+JF/q7puS3JLkT6rqqhXPCgAbY8kV8M1Jznf3M939cpIHk9yxZ00n+aGqqiQ/mOTrSS6udFIA2CBLAnxNkud2HV/YObfbB5P8bJLnk3whye9193dWMiEAbKAlAa59zvWe499M8rkkP5Hk55J8sKp++LteqOp0VZ2rqnMvvvjiZQ8LAJtiSYAvJLlu1/G12b7S3e3uJA/1tvNJvprkxr0v1N1nu3uru7dOnDhx2JkB4MhbEuDHk9xQVdfv/GDVnUke3rPm2SRvT5Kq+rEkP5PkmVUOCgCb5PhBC7r7YlXdm+TRJMeSPNDdT1XVPTvPn0nygSQfqaovZPst6/d390trnBsAjrQDA5wk3f1Ikkf2nDuz6+Pnk/zGakcDgM3lTlgAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABggwAAwQYAAYIMAAMECAAWCAAAPAAAEGgAECDAADBBgABiwKcFXdWlVPV9X5qrrvEmtuqarPVdVTVfXp1Y4JAJvl+EELqupYkvuT/HqSC0ker6qHu/tLu9a8LsmHktza3c9W1Y+ua2AA2ARLroBvTnK+u5/p7peTPJjkjj1r3pXkoe5+Nkm6+4XVjgkAm2VJgK9J8tyu4ws753Z7Y5LXV9WnquqJqnrPqgYEgE104FvQSWqfc73P67wlyduTfH+Sf6iqx7r7K//nhapOJzmdJCdPnrz8aQFgQyy5Ar6Q5Lpdx9cmeX6fNZ/s7m9190tJPpPkpr0v1N1nu3uru7dOnDhx2JkB4MhbEuDHk9xQVddX1VVJ7kzy8J41H0/yy1V1vKp+IMkvJPnyakcFgM1x4FvQ3X2xqu5N8miSY0ke6O6nquqenefPdPeXq+qTSZ5M8p0kH+7uL65zcAA4yqp777dzr4ytra0+d+7cyNcGgFWoqie6e+swn+tOWAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGLApwVd1aVU9X1fmquu8V1v18VX27qt65uhEBYPMcGOCqOpbk/iS3JTmV5K6qOnWJdX+c5NFVDwkAm2bJFfDNSc539zPd/XKSB5Pcsc+6303y0SQvrHA+ANhISwJ8TZLndh1f2Dn3P6rqmiTvSHJmdaMBwOZaEuDa51zvOf7TJO/v7m+/4gtVna6qc1V17sUXX1w6IwBsnOML1lxIct2u42uTPL9nzVaSB6sqSa5OcntVXezuj+1e1N1nk5xNkq2trb0RB4DvGUsC/HiSG6rq+iT/muTOJO/avaC7r//vj6vqI0n+em98AYD/dWCAu/tiVd2b7Z9uPpbkge5+qqru2Xne930B4DItuQJOdz+S5JE95/YNb3f/9qsfCwA2mzthAcAAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABiwKMBVdWtVPV1V56vqvn2e/62qenLn8dmqumn1owLA5jgwwFV1LMn9SW5LcirJXVV1as+yryb51e5+U5IPJDm76kEBYJMsuQK+Ocn57n6mu19O8mCSO3Yv6O7Pdvc3dg4fS3LtascEgM2yJMDXJHlu1/GFnXOX8t4kf/NqhgKATXd8wZra51zvu7DqbdkO8C9d4vnTSU4nycmTJxeOCACbZ8kV8IUk1+06vjbJ83sXVdWbknw4yR3d/W/7vVB3n+3ure7eOnHixGHmBYCNsCTAjye5oaqur6qrktyZ5OHdC6rqZJKHkry7u7+y+jEBYLMc+BZ0d1+sqnuTPJrkWJIHuvupqrpn5/kzSf4gyRuSfKiqkuRid2+tb2wAONqqe99v567d1tZWnzt3buRrA8AqVNUTh73gdCcsABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAQIMAAMWBbiqbq2qp6vqfFXdt8/zVVV/tvP8k1X15tWPCgCb48AAV9WxJPcnuS3JqSR3VdWpPctuS3LDzuN0kj9f8ZwAsFGWXAHfnOR8dz/T3S8neTDJHXvW3JHkL3rbY0leV1U/vuJZAWBjLAnwNUme23V8Yefc5a4BAHYcX7Cm9jnXh1iTqjqd7beok+Q/q+qLC74+l+/qJC9ND7Gh7O362Nv1sbfr8zOH/cQlAb6Q5Lpdx9cmef4Qa9LdZ5OcTZKqOtfdW5c1LYvY2/Wxt+tjb9fH3q5PVZ077OcueQv68SQ3VNX1VXVVkjuTPLxnzcNJ3rPz09BvTfLN7v7aYYcCgE134BVwd1+sqnuTPJrkWJIHuvupqrpn5/kzSR5JcnuS80n+I8nd6xsZAI6+JW9Bp7sfyXZkd587s+vjTvK+y/zaZy9zPcvZ2/Wxt+tjb9fH3q7Pofe2ttsJAFxJbkUJAAPWHmC3sVyfBXv7Wzt7+mRVfbaqbpqY8yg6aG93rfv5qvp2Vb3zSs53lC3Z26q6pao+V1VPVdWnr/SMR9WCvxN+pKo+UVWf39lbP6+zQFU9UFUvXOpXZw/dse5e2yPbP7T1z0l+KslVST6f5NSeNbcn+Zts/y7xW5P84zpn2pTHwr39xSSv3/n4Nnu7ur3dte7vsv3zEe+cnvsoPBb+uX1dki8lOblz/KPTcx+Fx8K9/f0kf7zz8YkkX09y1fTs/98fSX4lyZuTfPESzx+qY+u+AnYby/U5cG+7+7Pd/Y2dw8ey/fvZHGzJn9sk+d0kH03ywpUc7ohbsrfvSvJQdz+bJN1tf5dZsred5IeqqpL8YLYDfPHKjnn0dPdnsr1Xl3Kojq07wG5juT6Xu2/vzfa/0DjYgXtbVdckeUeSM+FyLPlz+8Ykr6+qT1XVE1X1nis23dG2ZG8/mORns32jpC8k+b3u/s6VGW+jHapji34N6VVY2W0s+S6L962q3pbtAP/SWifaHEv29k+TvL+7v719McFCS/b2eJK3JHl7ku9P8g9V9Vh3f2Xdwx1xS/b2N5N8LsmvJfnpJH9bVX/f3f++7uE23KE6tu4Ar+w2lnyXRftWVW9K8uEkt3X3v12h2Y66JXu7leTBnfheneT2qrrY3R+7MiMeWUv/Tnipu7+V5FtV9ZkkNyUR4Fe2ZG/vTvJHvf2Ny/NV9dUkNyb5pysz4sY6VMfW/Ra021iuz4F7W1UnkzyU5N2uHi7LgXvb3dd39092908m+askvyO+iyz5O+HjSX65qo5X1Q8k+YUkX77Ccx5FS/b22Wy/s5Cq+rFs/0cCz1zRKTfToTq21ivgdhvLtVm4t3+Q5A1JPrRzpXax3ZD9QAv3lkNYsrfd/eWq+mSSJ5N8J8mHu9v/nHaAhX9uP5DkI1X1hWy/bfr+7va/JB2gqv4yyS1Jrq6qC0n+MMlrklfXMXfCAoAB7oQFAAMEGAAGCDAADBBgABggwAAwQIABYIAAA8AAAQaAAf8FcB6nOpXlNKcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_m = KeyBERT()\n",
    "with torch.no_grad():\n",
    "    max_seq_len = 17\n",
    "    captions, features, urls = sample_coco_minibatch(small_data, batch_size=100, split='val')\n",
    "    print(features)\n",
    "    for i in range(10):\n",
    "        gen_caps = []\n",
    "        gen_caps.append(GenerateCaptions(features[i:i+1], captions[i:i+1], policyNet)[0])\n",
    "        gen_caps.append(GenerateCaptionsWithBeamSearch(features[i:i+1], captions[i:i+1], policyNet)[0][0][0])\n",
    "        gen_caps.append(GenerateCaptionsWithBeamSearchValueScoring(features[i:i+1], captions[i:i+1], policyNet)[0][0][0])\n",
    "        decoded_tru_caps = decode_captions(captions[i], data[\"idx_to_word\"])\n",
    "\n",
    "#         f = open(\"truth3.txt\", \"a\")\n",
    "#         f.write(decoded_tru_caps + \"\\n\")\n",
    "#         f.close()\n",
    "        \n",
    "#         decoded_gen_caps = decode_captions(gen_caps[0], data[\"idx_to_word\"])\n",
    "#         f = open(\"greedy3.txt\", \"a\")\n",
    "#         f.write(decoded_gen_caps + \"\\n\")\n",
    "#         f.close()\n",
    "        \n",
    "#         decoded_gen_caps = decode_captions(gen_caps[1], data[\"idx_to_word\"])\n",
    "#         f = open(\"beam3.txt\", \"a\")\n",
    "#         f.write(decoded_gen_caps + \"\\n\")\n",
    "#         f.close()\n",
    "        \n",
    "#         decoded_gen_caps = decode_captions(gen_caps[2], data[\"idx_to_word\"])\n",
    "#         f = open(\"policyvalue3.txt\", \"a\")\n",
    "#         f.write(decoded_gen_caps + \"\\n\")\n",
    "#         f.close()\n",
    "        try:\n",
    "            plt.imshow(image_from_url(urls[i]))\n",
    "            plt.show()\n",
    "        except:\n",
    "            pass\n",
    "        keyword = k_m.extract_keywords(decode_captions(gen_caps[2], data[\"idx_to_word\"]).replace(\"<START>\",\"\").replace(\"<END>\",\"\"))\n",
    "        print(urls[i])\n",
    "        print(decode_captions(gen_caps[2], data[\"idx_to_word\"]))\n",
    "        print(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLEU_score(gt_caption, sample_caption, w):\n",
    "    \"\"\"\n",
    "    gt_caption: string, ground-truth caption\n",
    "    sample_caption: string, your model's predicted caption\n",
    "    Returns unigram BLEU score.\n",
    "    \"\"\"\n",
    "    reference = [x for x in gt_caption.split(' ') \n",
    "                 if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "    hypothesis = [x for x in sample_caption.split(' ') \n",
    "                  if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = [w])\n",
    "    return BLEUscore\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"\n",
    "    model: CaptioningRNN model\n",
    "    Prints unigram BLEU score averaged over 1000 training and val examples.\n",
    "    \"\"\"\n",
    "    BLEUscores = {}\n",
    "    for split in ['train', 'val']:\n",
    "        minibatch = sample_coco_minibatch(data, split=split, batch_size=1000)\n",
    "        gt_captions, features, urls = minibatch\n",
    "        gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
    "\n",
    "        sample_captions = model.sample(features)\n",
    "        sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n",
    "\n",
    "        total_score = 0.0\n",
    "        for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n",
    "            total_score += BLEU_score(gt_caption, sample_caption)\n",
    "\n",
    "        BLEUscores[split] = total_score / len(sample_captions)\n",
    "\n",
    "    for split in BLEUscores:\n",
    "        print('Average BLEU score for %s: %f' % (split, BLEUscores[split]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps0 = []\n",
    "caps1 = []\n",
    "caps2 = []\n",
    "caps3 = []\n",
    "f = open(\"results/truth3.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps0.append(x)\n",
    "f = open(\"results/greedy3.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps1.append(x)\n",
    "f = open(\"results/beam3.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps2.append(x)\n",
    "f = open(\"results/policyvalue3.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps3.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy BLEU-1 : 0.24624043997583175\n",
      "Beam BLEU-1 : 0.2540224578441918\n",
      "Agent BLEU-1 : 0.2563426767182271\n",
      "\n",
      "Greedy BLEU-2 : 0.09065434672351722\n",
      "Beam BLEU-2 : 0.0902509189133115\n",
      "Agent BLEU-2 : 0.09155483470073025\n",
      "\n",
      "Greedy BLEU-3 : 0.039722450216904244\n",
      "Beam BLEU-3 : 0.037689433253779\n",
      "Agent BLEU-3 : 0.03852101647793212\n",
      "\n",
      "Greedy BLEU-4 : 0.019965843321998366\n",
      "Beam BLEU-4 : 0.017952085683363046\n",
      "Agent BLEU-4 : 0.018542079795694805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b1, b2, b3 = 0, 0, 0\n",
    "for w in range(1, 5):\n",
    "    for i in range(len(caps0)):\n",
    "        b1 += BLEU_score(caps0[i], caps1[i], w)\n",
    "        b2 += BLEU_score(caps0[i], caps2[i], w)\n",
    "        b3 += BLEU_score(caps0[i], caps3[i], w)\n",
    "    b1 /= len(caps0)\n",
    "    b2 /= len(caps0)\n",
    "    b3 /= len(caps0)\n",
    "    print(\"Greedy BLEU-\" + str(w), \":\", b1)\n",
    "    print(\"Beam BLEU-\" + str(w), \":\", b2)\n",
    "    print(\"Agent BLEU-\" + str(w), \":\", b3)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of references is 1000\n",
      "{'Bleu_1': 0.2727798112116863, 'Bleu_2': 0.13207907633440683, 'Bleu_3': 0.0704492797131235, 'Bleu_4': 0.03961546368562404, 'ROUGE_L': 0.2611785262449472}\n"
     ]
    }
   ],
   "source": [
    "ref, hypo = metrics.load_textfiles(caps0, caps3)\n",
    "print(metrics.score(ref, hypo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = True\n",
    "\n",
    "policyNetwork = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(policyNetwork.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "if pretrained:\n",
    "    policyNetwork.load_state_dict(torch.load('models/policyNetwork.pt'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000\n",
      "epoch: 250000 loss: 46.593292236328125\n",
      "250001\n",
      "250002\n",
      "250003\n",
      "epoch: 250003 loss: 46.031776428222656\n",
      "250004\n",
      "250005\n",
      "250006\n",
      "250007\n",
      "250008\n",
      "250009\n",
      "250010\n",
      "250011\n",
      "250012\n",
      "250013\n",
      "250014\n",
      "epoch: 250014 loss: 45.665836334228516\n",
      "250015\n",
      "250016\n",
      "250017\n",
      "250018\n",
      "250019\n",
      "250020\n",
      "250021\n",
      "250022\n",
      "250023\n",
      "250024\n",
      "250025\n",
      "250026\n",
      "epoch: 250026 loss: 45.38121032714844\n",
      "250027\n",
      "250028\n",
      "250029\n",
      "250030\n",
      "250031\n",
      "epoch: 250031 loss: 45.21809768676758\n",
      "250032\n",
      "250033\n",
      "250034\n",
      "250035\n",
      "250036\n",
      "epoch: 250036 loss: 44.29335021972656\n",
      "250037\n",
      "250038\n",
      "250039\n",
      "250040\n",
      "250041\n",
      "250042\n",
      "250043\n",
      "250044\n",
      "250045\n",
      "250046\n",
      "250047\n",
      "250048\n",
      "250049\n",
      "250050\n",
      "250051\n",
      "250052\n",
      "250053\n",
      "250054\n",
      "250055\n",
      "250056\n",
      "epoch: 250056 loss: 44.239627838134766\n",
      "250057\n",
      "250058\n",
      "250059\n",
      "250060\n",
      "250061\n",
      "250062\n",
      "epoch: 250062 loss: 43.287105560302734\n",
      "250063\n",
      "250064\n",
      "250065\n",
      "250066\n",
      "250067\n",
      "250068\n",
      "250069\n",
      "250070\n",
      "250071\n",
      "250072\n",
      "250073\n",
      "250074\n",
      "250075\n",
      "250076\n",
      "250077\n",
      "250078\n",
      "250079\n",
      "250080\n",
      "250081\n",
      "250082\n",
      "250083\n",
      "250084\n",
      "250085\n",
      "250086\n",
      "250087\n",
      "250088\n",
      "250089\n",
      "250090\n",
      "250091\n",
      "250092\n",
      "250093\n",
      "250094\n",
      "250095\n",
      "250096\n",
      "epoch: 250096 loss: 42.179588317871094\n",
      "250097\n",
      "250098\n",
      "250099\n",
      "250100\n",
      "250101\n",
      "250102\n",
      "250103\n",
      "250104\n",
      "250105\n",
      "250106\n",
      "250107\n",
      "250108\n",
      "250109\n",
      "250110\n",
      "250111\n",
      "250112\n",
      "250113\n",
      "250114\n",
      "250115\n",
      "250116\n",
      "250117\n",
      "250118\n",
      "250119\n",
      "250120\n",
      "250121\n",
      "250122\n",
      "250123\n",
      "250124\n",
      "250125\n",
      "250126\n",
      "250127\n",
      "250128\n",
      "250129\n",
      "250130\n",
      "250131\n",
      "250132\n",
      "epoch: 250132 loss: 41.13591003417969\n",
      "250133\n",
      "250134\n",
      "250135\n",
      "250136\n",
      "250137\n",
      "250138\n",
      "250139\n",
      "250140\n",
      "250141\n",
      "250142\n",
      "250143\n",
      "250144\n",
      "250145\n",
      "250146\n",
      "250147\n",
      "250148\n",
      "250149\n",
      "250150\n",
      "250151\n",
      "250152\n",
      "250153\n",
      "250154\n",
      "250155\n",
      "250156\n",
      "250157\n",
      "250158\n",
      "250159\n",
      "250160\n",
      "250161\n",
      "250162\n",
      "250163\n",
      "250164\n",
      "250165\n",
      "250166\n",
      "250167\n",
      "250168\n",
      "250169\n",
      "250170\n",
      "epoch: 250170 loss: 40.767581939697266\n",
      "250171\n",
      "250172\n",
      "250173\n",
      "250174\n",
      "250175\n",
      "250176\n",
      "250177\n",
      "250178\n",
      "250179\n",
      "250180\n",
      "250181\n",
      "250182\n",
      "250183\n",
      "250184\n",
      "250185\n",
      "250186\n",
      "250187\n",
      "250188\n",
      "250189\n",
      "250190\n",
      "250191\n",
      "250192\n",
      "250193\n",
      "250194\n",
      "250195\n",
      "250196\n",
      "250197\n",
      "250198\n",
      "250199\n",
      "250200\n",
      "250201\n",
      "250202\n",
      "250203\n",
      "250204\n",
      "250205\n",
      "250206\n",
      "250207\n",
      "250208\n",
      "250209\n",
      "250210\n",
      "250211\n",
      "250212\n",
      "250213\n",
      "250214\n",
      "250215\n",
      "250216\n",
      "250217\n",
      "epoch: 250217 loss: 39.796356201171875\n",
      "250218\n",
      "250219\n",
      "250220\n",
      "250221\n",
      "250222\n",
      "250223\n",
      "250224\n",
      "250225\n",
      "250226\n",
      "250227\n",
      "250228\n",
      "250229\n",
      "250230\n",
      "250231\n",
      "250232\n",
      "250233\n",
      "250234\n",
      "250235\n",
      "250236\n",
      "250237\n",
      "250238\n",
      "250239\n",
      "250240\n",
      "250241\n",
      "250242\n",
      "250243\n",
      "epoch: 250243 loss: 39.255611419677734\n",
      "250244\n",
      "250245\n",
      "250246\n",
      "250247\n",
      "250248\n",
      "250249\n",
      "250250\n",
      "250251\n",
      "250252\n",
      "250253\n",
      "250254\n",
      "250255\n",
      "250256\n",
      "250257\n",
      "250258\n",
      "250259\n",
      "250260\n",
      "250261\n",
      "250262\n",
      "250263\n",
      "epoch: 250263 loss: 38.966949462890625\n",
      "250264\n",
      "250265\n",
      "250266\n",
      "250267\n",
      "250268\n",
      "250269\n",
      "250270\n",
      "250271\n",
      "epoch: 250271 loss: 38.54885482788086\n",
      "250272\n",
      "250273\n",
      "250274\n",
      "250275\n",
      "250276\n",
      "250277\n",
      "250278\n",
      "250279\n",
      "250280\n",
      "250281\n",
      "250282\n",
      "250283\n",
      "250284\n",
      "250285\n",
      "250286\n",
      "250287\n",
      "250288\n",
      "250289\n",
      "250290\n",
      "250291\n",
      "250292\n",
      "250293\n",
      "250294\n",
      "epoch: 250294 loss: 37.972206115722656\n",
      "250295\n",
      "250296\n",
      "250297\n",
      "250298\n",
      "250299\n",
      "250300\n",
      "250301\n",
      "250302\n",
      "250303\n",
      "250304\n",
      "250305\n",
      "250306\n",
      "250307\n",
      "epoch: 250307 loss: 37.682613372802734\n",
      "250308\n",
      "250309\n",
      "250310\n",
      "250311\n",
      "250312\n",
      "250313\n",
      "250314\n",
      "250315\n",
      "250316\n",
      "250317\n",
      "250318\n",
      "250319\n",
      "250320\n",
      "250321\n",
      "250322\n",
      "250323\n",
      "250324\n",
      "250325\n",
      "250326\n",
      "250327\n",
      "250328\n",
      "250329\n",
      "epoch: 250329 loss: 37.37826919555664\n",
      "250330\n",
      "250331\n",
      "250332\n",
      "250333\n",
      "250334\n",
      "250335\n",
      "250336\n",
      "250337\n",
      "250338\n",
      "250339\n",
      "250340\n",
      "250341\n",
      "250342\n",
      "250343\n",
      "250344\n",
      "250345\n",
      "250346\n",
      "250347\n",
      "250348\n",
      "250349\n",
      "250350\n",
      "250351\n",
      "250352\n",
      "250353\n",
      "250354\n",
      "250355\n",
      "250356\n",
      "250357\n",
      "250358\n",
      "250359\n",
      "250360\n",
      "250361\n",
      "250362\n",
      "epoch: 250362 loss: 37.245635986328125\n",
      "250363\n",
      "250364\n",
      "250365\n",
      "250366\n",
      "250367\n",
      "250368\n",
      "250369\n",
      "250370\n",
      "250371\n",
      "250372\n",
      "250373\n",
      "250374\n",
      "250375\n",
      "250376\n",
      "250377\n",
      "250378\n",
      "250379\n",
      "250380\n",
      "250381\n",
      "250382\n",
      "250383\n",
      "250384\n",
      "250385\n",
      "250386\n",
      "250387\n",
      "250388\n",
      "250389\n",
      "250390\n",
      "250391\n",
      "epoch: 250391 loss: 37.063865661621094\n",
      "250392\n",
      "250393\n",
      "250394\n",
      "250395\n",
      "250396\n",
      "250397\n",
      "250398\n",
      "250399\n",
      "250400\n",
      "250401\n",
      "250402\n",
      "250403\n",
      "250404\n",
      "250405\n",
      "250406\n",
      "250407\n",
      "250408\n",
      "250409\n",
      "250410\n",
      "250411\n",
      "epoch: 250411 loss: 37.01962661743164\n",
      "250412\n",
      "250413\n",
      "250414\n",
      "250415\n",
      "250416\n",
      "250417\n",
      "250418\n",
      "epoch: 250418 loss: 35.9730339050293\n",
      "250419\n",
      "250420\n",
      "250421\n",
      "250422\n",
      "250423\n",
      "250424\n",
      "250425\n",
      "250426\n",
      "250427\n",
      "250428\n",
      "250429\n",
      "250430\n",
      "250431\n",
      "250432\n",
      "250433\n",
      "250434\n",
      "250435\n",
      "250436\n",
      "250437\n",
      "250438\n",
      "250439\n",
      "250440\n",
      "250441\n",
      "250442\n",
      "250443\n",
      "250444\n",
      "250445\n",
      "250446\n",
      "250447\n",
      "250448\n",
      "250449\n",
      "250450\n",
      "250451\n",
      "250452\n",
      "250453\n",
      "250454\n",
      "250455\n",
      "250456\n",
      "250457\n",
      "250458\n",
      "250459\n",
      "250460\n",
      "250461\n",
      "250462\n",
      "250463\n",
      "250464\n",
      "250465\n",
      "250466\n",
      "250467\n",
      "250468\n",
      "250469\n",
      "250470\n",
      "250471\n",
      "250472\n",
      "250473\n",
      "250474\n",
      "250475\n",
      "250476\n",
      "250477\n",
      "250478\n",
      "250479\n",
      "250480\n",
      "250481\n",
      "250482\n",
      "250483\n",
      "250484\n",
      "250485\n",
      "250486\n",
      "250487\n",
      "250488\n",
      "250489\n",
      "250490\n",
      "250491\n",
      "250492\n",
      "250493\n",
      "250494\n",
      "250495\n",
      "250496\n",
      "250497\n",
      "250498\n",
      "250499\n",
      "250500\n",
      "250501\n",
      "250502\n",
      "250503\n",
      "250504\n",
      "250505\n",
      "250506\n",
      "250507\n",
      "250508\n",
      "250509\n",
      "250510\n",
      "250511\n",
      "250512\n",
      "250513\n",
      "250514\n",
      "250515\n",
      "250516\n",
      "250517\n",
      "250518\n",
      "250519\n",
      "250520\n",
      "250521\n",
      "250522\n",
      "250523\n",
      "250524\n",
      "250525\n",
      "250526\n",
      "250527\n",
      "250528\n",
      "250529\n",
      "250530\n",
      "250531\n",
      "250532\n",
      "250533\n",
      "250534\n",
      "250535\n",
      "250536\n",
      "250537\n",
      "250538\n",
      "250539\n",
      "epoch: 250539 loss: 35.954750061035156\n",
      "250540\n",
      "250541\n",
      "250542\n",
      "250543\n",
      "250544\n",
      "250545\n",
      "250546\n",
      "250547\n",
      "250548\n",
      "epoch: 250548 loss: 35.527061462402344\n",
      "250549\n",
      "250550\n",
      "250551\n",
      "250552\n",
      "250553\n",
      "250554\n",
      "250555\n",
      "250556\n",
      "250557\n",
      "250558\n",
      "250559\n",
      "250560\n",
      "250561\n",
      "250562\n",
      "250563\n",
      "250564\n",
      "250565\n",
      "250566\n",
      "250567\n",
      "250568\n",
      "250569\n",
      "250570\n",
      "250571\n",
      "250572\n",
      "250573\n",
      "250574\n",
      "250575\n",
      "250576\n",
      "250577\n",
      "250578\n",
      "250579\n",
      "250580\n",
      "250581\n",
      "250582\n",
      "250583\n",
      "250584\n",
      "250585\n",
      "250586\n",
      "250587\n",
      "250588\n",
      "250589\n",
      "250590\n",
      "250591\n",
      "250592\n",
      "250593\n",
      "250594\n",
      "250595\n",
      "250596\n",
      "250597\n",
      "250598\n",
      "250599\n",
      "250600\n",
      "epoch: 250600 loss: 34.90866470336914\n",
      "250601\n",
      "250602\n",
      "250603\n",
      "250604\n",
      "250605\n",
      "250606\n",
      "250607\n",
      "250608\n",
      "250609\n",
      "250610\n",
      "250611\n",
      "250612\n",
      "250613\n",
      "250614\n",
      "250615\n",
      "250616\n",
      "250617\n",
      "250618\n",
      "250619\n",
      "250620\n",
      "250621\n",
      "250622\n",
      "250623\n",
      "250624\n",
      "250625\n",
      "250626\n",
      "250627\n",
      "250628\n",
      "250629\n",
      "250630\n",
      "250631\n",
      "250632\n",
      "250633\n",
      "250634\n",
      "250635\n",
      "250636\n",
      "250637\n",
      "250638\n",
      "250639\n",
      "250640\n",
      "250641\n",
      "250642\n",
      "250643\n",
      "epoch: 250643 loss: 33.560420989990234\n",
      "250644\n",
      "250645\n",
      "250646\n",
      "250647\n",
      "250648\n",
      "250649\n",
      "250650\n",
      "250651\n",
      "250652\n",
      "250653\n",
      "250654\n",
      "250655\n",
      "250656\n",
      "250657\n",
      "250658\n",
      "250659\n",
      "250660\n",
      "250661\n",
      "250662\n",
      "250663\n",
      "250664\n",
      "250665\n",
      "250666\n",
      "250667\n",
      "250668\n",
      "250669\n",
      "250670\n",
      "250671\n",
      "250672\n",
      "250673\n",
      "250674\n",
      "250675\n",
      "250676\n",
      "250677\n",
      "250678\n",
      "250679\n",
      "250680\n",
      "250681\n",
      "250682\n",
      "250683\n",
      "250684\n",
      "250685\n",
      "250686\n",
      "250687\n",
      "250688\n",
      "250689\n",
      "250690\n",
      "250691\n",
      "250692\n",
      "250693\n",
      "250694\n",
      "250695\n",
      "250696\n",
      "250697\n",
      "250698\n",
      "250699\n",
      "250700\n",
      "250701\n",
      "250702\n",
      "250703\n",
      "250704\n",
      "250705\n",
      "250706\n",
      "250707\n",
      "250708\n",
      "250709\n",
      "250710\n",
      "250711\n",
      "250712\n",
      "250713\n",
      "250714\n",
      "250715\n",
      "250716\n",
      "250717\n",
      "250718\n",
      "250719\n",
      "250720\n",
      "250721\n",
      "250722\n",
      "250723\n",
      "250724\n",
      "250725\n",
      "250726\n",
      "250727\n",
      "250728\n",
      "250729\n",
      "epoch: 250729 loss: 32.77178192138672\n",
      "250730\n",
      "250731\n",
      "250732\n",
      "250733\n",
      "250734\n",
      "250735\n",
      "250736\n",
      "250737\n",
      "250738\n",
      "250739\n",
      "250740\n",
      "250741\n",
      "250742\n",
      "250743\n",
      "250744\n",
      "250745\n",
      "250746\n",
      "250747\n",
      "250748\n",
      "250749\n",
      "250750\n",
      "250751\n",
      "250752\n",
      "250753\n",
      "250754\n",
      "250755\n",
      "250756\n",
      "250757\n",
      "250758\n",
      "250759\n",
      "250760\n",
      "250761\n",
      "250762\n",
      "250763\n",
      "250764\n",
      "250765\n",
      "250766\n",
      "250767\n",
      "250768\n",
      "250769\n",
      "250770\n",
      "250771\n",
      "250772\n",
      "250773\n",
      "250774\n",
      "250775\n",
      "250776\n",
      "250777\n",
      "250778\n",
      "250779\n",
      "250780\n",
      "250781\n",
      "250782\n",
      "250783\n",
      "250784\n",
      "250785\n",
      "250786\n",
      "250787\n",
      "250788\n",
      "250789\n",
      "250790\n",
      "250791\n",
      "250792\n",
      "250793\n",
      "250794\n",
      "250795\n",
      "250796\n",
      "250797\n",
      "250798\n",
      "250799\n",
      "250800\n",
      "250801\n",
      "250802\n",
      "250803\n",
      "250804\n",
      "250805\n",
      "250806\n",
      "250807\n",
      "250808\n",
      "250809\n",
      "250810\n",
      "250811\n",
      "250812\n",
      "250813\n",
      "250814\n",
      "250815\n",
      "250816\n",
      "250817\n",
      "250818\n",
      "250819\n",
      "250820\n",
      "250821\n",
      "250822\n",
      "250823\n",
      "250824\n",
      "250825\n",
      "250826\n",
      "250827\n",
      "250828\n",
      "250829\n",
      "250830\n",
      "250831\n",
      "250832\n",
      "250833\n",
      "250834\n",
      "250835\n",
      "250836\n",
      "250837\n",
      "250838\n",
      "250839\n",
      "250840\n",
      "250841\n",
      "250842\n",
      "250843\n",
      "250844\n",
      "250845\n",
      "250846\n",
      "250847\n",
      "250848\n",
      "250849\n",
      "250850\n",
      "250851\n",
      "250852\n",
      "250853\n",
      "250854\n",
      "250855\n",
      "250856\n",
      "250857\n",
      "250858\n",
      "250859\n",
      "250860\n",
      "250861\n",
      "250862\n",
      "250863\n",
      "250864\n",
      "250865\n",
      "250866\n",
      "250867\n",
      "250868\n",
      "250869\n",
      "250870\n",
      "250871\n",
      "250872\n",
      "250873\n",
      "250874\n",
      "250875\n",
      "250876\n",
      "250877\n",
      "250878\n",
      "250879\n",
      "250880\n",
      "250881\n",
      "epoch: 250881 loss: 32.06758499145508\n",
      "250882\n",
      "250883\n",
      "250884\n",
      "250885\n",
      "250886\n",
      "250887\n",
      "250888\n",
      "250889\n",
      "250890\n",
      "250891\n",
      "250892\n",
      "250893\n",
      "250894\n",
      "250895\n",
      "250896\n",
      "250897\n",
      "250898\n",
      "250899\n",
      "250900\n",
      "250901\n",
      "250902\n",
      "250903\n",
      "250904\n",
      "250905\n",
      "250906\n",
      "250907\n",
      "250908\n",
      "250909\n",
      "250910\n",
      "250911\n",
      "250912\n",
      "250913\n",
      "250914\n",
      "250915\n",
      "250916\n",
      "250917\n",
      "250918\n",
      "250919\n",
      "250920\n",
      "250921\n",
      "250922\n",
      "250923\n",
      "250924\n",
      "250925\n",
      "250926\n",
      "250927\n",
      "250928\n",
      "250929\n",
      "250930\n",
      "250931\n",
      "250932\n",
      "250933\n",
      "250934\n",
      "250935\n",
      "250936\n",
      "250937\n",
      "250938\n",
      "250939\n",
      "250940\n",
      "250941\n",
      "250942\n",
      "250943\n",
      "250944\n",
      "250945\n",
      "250946\n",
      "250947\n",
      "250948\n",
      "250949\n",
      "250950\n",
      "250951\n",
      "250952\n",
      "250953\n",
      "250954\n",
      "250955\n",
      "250956\n",
      "250957\n",
      "250958\n",
      "250959\n",
      "250960\n",
      "250961\n",
      "250962\n",
      "250963\n",
      "250964\n",
      "250965\n",
      "250966\n",
      "250967\n",
      "250968\n",
      "250969\n",
      "250970\n",
      "250971\n",
      "250972\n",
      "250973\n",
      "250974\n",
      "250975\n",
      "250976\n",
      "250977\n",
      "250978\n",
      "250979\n",
      "250980\n",
      "250981\n",
      "250982\n",
      "250983\n",
      "250984\n",
      "250985\n",
      "250986\n",
      "250987\n",
      "250988\n",
      "250989\n",
      "250990\n",
      "250991\n",
      "250992\n",
      "250993\n",
      "epoch: 250993 loss: 32.0516357421875\n",
      "250994\n",
      "250995\n",
      "250996\n",
      "250997\n",
      "250998\n",
      "250999\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "bestLoss = 50\n",
    "#0.006700546946376562\n",
    "\n",
    "for epoch in range(250000, 251000):\n",
    "    print(epoch)\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    captions_in = torch.tensor(captions[:, :-1], device=device).long()\n",
    "    captions_ou = torch.tensor(captions[:, 1:], device=device).long()\n",
    "    output = policyNetwork(features, captions_in)\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        caplen = np.nonzero(captions[i] == 2)[0][0] + 1\n",
    "        loss += (caplen/batch_size)*criterion(output[i][:caplen], captions_ou[i][:caplen])\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(policyNetwork.state_dict(), \"policyNetwork.pt\")\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Reward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardNetwork = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "optimizer = optim.Adam(rewardNetwork.parameters(), lr=0.001)\n",
    "\n",
    "# https://cs230-stanford.github.io/pytorch-nlp.html#writing-a-custom-loss-function\n",
    "def VisualSemanticEmbeddingLoss(visuals, semantics):\n",
    "    beta = 0.2\n",
    "    N, D = visuals.shape\n",
    "    \n",
    "    visloss = torch.mm(visuals, semantics.t())\n",
    "    visloss = visloss - torch.diag(visloss).unsqueeze(1)\n",
    "    visloss = visloss + (beta/N)*(torch.ones((N, N)).to(device) - torch.eye(N).to(device))\n",
    "    visloss = F.relu(visloss)\n",
    "    visloss = torch.sum(visloss)/N\n",
    "    \n",
    "    semloss = torch.mm(semantics, visuals.t())\n",
    "    semloss = semloss - torch.diag(semloss).unsqueeze(1)\n",
    "    semloss = semloss + (beta/N)*(torch.ones((N, N)).to(device) - torch.eye(N).to(device))\n",
    "    semloss = F.relu(semloss)\n",
    "    semloss = torch.sum(semloss)/N\n",
    "    \n",
    "    return visloss + semloss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "epoch: 0 loss: 251.90911865234375\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "epoch: 127 loss: 247.28048706054688\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "epoch: 232 loss: 238.68353271484375\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "epoch: 270 loss: 236.0042266845703\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "epoch: 288 loss: 227.09605407714844\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "epoch: 318 loss: 223.49737548828125\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "epoch: 362 loss: 208.4512939453125\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "epoch: 431 loss: 191.86729431152344\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "epoch: 504 loss: 175.0537109375\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "epoch: 571 loss: 174.98439025878906\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "epoch: 688 loss: 166.6080780029297\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "epoch: 694 loss: 156.5119171142578\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "epoch: 745 loss: 143.49252319335938\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "epoch: 796 loss: 139.7491912841797\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "epoch: 959 loss: 132.13616943359375\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "epoch: 997 loss: 132.05923461914062\n",
      "998\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "bestLoss = 10000\n",
    "\n",
    "for epoch in range(1000):\n",
    "    print(epoch)\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float()\n",
    "    captions = torch.tensor(captions, device=device).long()\n",
    "    ve, se = rewardNetwork(features, captions)\n",
    "    loss = VisualSemanticEmbeddingLoss(ve, se)\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(rewardNetwork.state_dict(), \"rewardNetwork.pt\")\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    rewardNetwork.rewrnn.hidden_cell.detach_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRewards(features, captions, model):\n",
    "    visEmbeds, semEmbeds = model(features, captions)\n",
    "    visEmbeds = F.normalize(visEmbeds, p=2, dim=1) \n",
    "    semEmbeds = F.normalize(semEmbeds, p=2, dim=1) \n",
    "    rewards = torch.sum(visEmbeds*semEmbeds, axis=1).unsqueeze(1)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RewardNetwork(\n",
      "  (rewrnn): RewardNetworkRNN(\n",
      "    (caption_embedding): Embedding(1004, 512)\n",
      "    (gru): GRU(512, 512)\n",
      "  )\n",
      "  (visual_embed): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (semantic_embed): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "PolicyNetwork(\n",
      "  (caption_embedding): Embedding(1004, 512)\n",
      "  (cnn2linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (lstm): LSTM(512, 512, batch_first=True)\n",
      "  (linear2vocab): Linear(in_features=512, out_features=1004, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ValueNetwork(\n",
       "  (valrnn): ValueNetworkRNN(\n",
       "    (caption_embedding): Embedding(1004, 512)\n",
       "    (lstm): LSTM(512, 512)\n",
       "  )\n",
       "  (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewardNet = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "rewardNet.load_state_dict(torch.load('rewardNetwork.pt'))\n",
    "for param in rewardNet.parameters():\n",
    "    param.require_grad = False\n",
    "print(rewardNet)\n",
    "\n",
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet.load_state_dict(torch.load('policyNetwork.pt'))\n",
    "for param in policyNet.parameters():\n",
    "    param.require_grad = False\n",
    "print(policyNet)\n",
    "\n",
    "valueNetwork = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(valueNetwork.parameters(), lr=0.0001)\n",
    "valueNetwork.train(mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.2978333532810211\n",
      "1\n",
      "2\n",
      "epoch: 2 loss: 0.24407081305980682\n",
      "3\n",
      "4\n",
      "5\n",
      "epoch: 5 loss: 0.2090148776769638\n",
      "6\n",
      "7\n",
      "epoch: 7 loss: 0.1365364044904709\n",
      "8\n",
      "epoch: 8 loss: 0.11033648252487183\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "epoch: 13 loss: 0.08113570511341095\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "epoch: 18 loss: 0.07739177346229553\n",
      "19\n",
      "epoch: 19 loss: 0.06010852009057999\n",
      "20\n",
      "epoch: 20 loss: 0.055811185389757156\n",
      "21\n",
      "epoch: 21 loss: 0.048952627927064896\n",
      "22\n",
      "23\n",
      "24\n",
      "epoch: 24 loss: 0.03812519460916519\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "epoch: 30 loss: 0.03199620544910431\n",
      "31\n",
      "epoch: 31 loss: 0.030315712094306946\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "epoch: 40 loss: 0.02730732224881649\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "epoch: 44 loss: 0.026722168549895287\n",
      "45\n",
      "46\n",
      "epoch: 46 loss: 0.01963929831981659\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "epoch: 51 loss: 0.012558701448142529\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "epoch: 60 loss: 0.012210254557430744\n",
      "61\n",
      "epoch: 61 loss: 0.011992527171969414\n",
      "62\n",
      "63\n",
      "64\n",
      "epoch: 64 loss: 0.011972343549132347\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "epoch: 70 loss: 0.011026845313608646\n",
      "71\n",
      "epoch: 71 loss: 0.009631924331188202\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "epoch: 77 loss: 0.008422953076660633\n",
      "78\n",
      "79\n",
      "epoch: 79 loss: 0.004347699694335461\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "epoch: 97 loss: 0.004116229247301817\n",
      "98\n",
      "epoch: 98 loss: 0.0032323941122740507\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "epoch: 104 loss: 0.002435885136947036\n",
      "105\n",
      "epoch: 105 loss: 0.0023236251436173916\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "epoch: 109 loss: 0.0018931045196950436\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "epoch: 119 loss: 0.0014849314466118813\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "epoch: 125 loss: 0.0013979875948280096\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "epoch: 129 loss: 0.0013176256325095892\n",
      "130\n",
      "131\n",
      "epoch: 131 loss: 0.0013071835273876786\n",
      "132\n",
      "133\n",
      "epoch: 133 loss: 0.0012796048540621996\n",
      "134\n",
      "epoch: 134 loss: 0.0012730417074635625\n",
      "135\n",
      "136\n",
      "137\n",
      "epoch: 137 loss: 0.0011892338516190648\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "epoch: 143 loss: 0.001072246697731316\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "epoch: 148 loss: 0.0010146384593099356\n",
      "149\n",
      "150\n",
      "151\n",
      "epoch: 151 loss: 0.0008316603489220142\n",
      "152\n",
      "epoch: 152 loss: 0.0007385142962448299\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "epoch: 158 loss: 0.0005285709630697966\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "epoch: 189 loss: 0.000508167955558747\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "epoch: 197 loss: 0.0004352701362222433\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-81423a4d0fb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "bestLoss = 10000\n",
    "max_seq_len = 17\n",
    "\n",
    "for epoch in range(1000):\n",
    "    print(epoch)\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float()\n",
    "    \n",
    "    # Generate captions using the policy network\n",
    "    captions = GenerateCaptions(features, captions, policyNet)\n",
    "    \n",
    "    # Compute the reward of the generated caption using reward network\n",
    "    rewards = GetRewards(features, captions, rewardNet)\n",
    "    \n",
    "    # Compute the value of a random state in the generation process\n",
    "#     print(features.shape, captions[:, :random.randint(1, 17)].shape)\n",
    "    values = valueNetwork(features, captions[:, :random.randint(1, 17)])\n",
    "    \n",
    "    # Compute the loss for the value and the reward\n",
    "    loss = criterion(values, rewards)\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(valueNetwork.state_dict(), \"valueNetwork.pt\")\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    valueNetwork.valrnn.hidden_cell[0].detach_()\n",
    "    valueNetwork.valrnn.hidden_cell[1].detach_()\n",
    "    rewardNet.rewrnn.hidden_cell.detach_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "Advantage Actor Critic Model for Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvantageActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, valueNet, policyNet):\n",
    "        super(AdvantageActorCriticNetwork, self).__init__()\n",
    "\n",
    "        self.valueNet = valueNet #RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "        self.policyNet = policyNet #PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # Get value from value network\n",
    "        values = self.valueNet(features, captions)\n",
    "        # Get action probabilities from policy network\n",
    "        probs = self.policyNet(features.unsqueeze(0), captions)[:, -1:, :]        \n",
    "        return values, probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardNet = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "valueNet = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "\n",
    "rewardNet.load_state_dict(torch.load('rewardNetwork.pt'))\n",
    "policyNet.load_state_dict(torch.load('policyNetwork.pt'))\n",
    "valueNet.load_state_dict(torch.load('valueNetwork.pt'))\n",
    "\n",
    "a2cNetwork = AdvantageActorCriticNetwork(valueNet, policyNet)\n",
    "optimizer = optim.Adam(a2cNetwork.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : -0.007387125713285064\n",
      "1 : 0.04488514754921198\n",
      "2 : -0.06401610735338181\n",
      "3 : 0.05547436216729692\n",
      "4 : -0.003837803754140631\n",
      "5 : 0.03098210810130695\n",
      "6 : -0.0022535616502864313\n",
      "7 : -0.06710768283810466\n",
      "8 : -0.05291894957190377\n",
      "9 : -0.010877899546176208\n",
      "10 : -0.0625397466123104\n",
      "11 : -0.01266534309834242\n",
      "12 : -0.014296415238641206\n",
      "13 : -0.10871602222323418\n",
      "14 : -0.054213504854124045\n",
      "15 : 0.07687334474176168\n",
      "16 : -0.04758633135352283\n",
      "17 : 0.02093416415154934\n",
      "18 : -0.02663892600685358\n",
      "19 : -0.04710544722620399\n",
      "20 : 0.009828876378014686\n",
      "21 : 0.04300703879562207\n",
      "22 : -0.0007733814098173767\n",
      "23 : -0.052076150616630915\n",
      "24 : -0.014234895121899169\n",
      "25 : -0.07356770869228058\n",
      "26 : 0.00901621376397088\n",
      "27 : -0.03975281740713398\n",
      "28 : 0.016795617592288183\n",
      "29 : -0.0014798063450143673\n",
      "30 : -0.022788259154185656\n",
      "31 : -0.04661788479425013\n",
      "32 : -0.003521186858415604\n",
      "33 : -0.02608041261555627\n",
      "34 : 0.051183212338946765\n",
      "35 : 0.002820358256576583\n",
      "36 : 0.014380510864430108\n",
      "37 : 0.006490728957578544\n",
      "38 : -0.049235158099327235\n",
      "39 : -0.0450525302614551\n",
      "40 : 0.021601759117038453\n",
      "41 : 0.007438415475189674\n",
      "42 : -0.04880731369485147\n",
      "43 : 0.03880422646179795\n",
      "44 : 0.0073351924773305665\n",
      "45 : 0.02472762867691927\n",
      "46 : 0.05280178461689502\n",
      "47 : -0.057745166250970204\n",
      "48 : 0.005554441502317787\n",
      "49 : -0.015021782674011773\n",
      "50 : -0.019807437289273366\n",
      "51 : -0.01095512614847394\n",
      "52 : -0.06746063057798891\n",
      "53 : 0.01182059964339715\n",
      "54 : 0.015528415835433403\n",
      "55 : 0.03366283883224241\n",
      "56 : -0.01565923281305004\n",
      "57 : -0.040700020513031634\n",
      "58 : 0.021462685952428732\n",
      "59 : -0.08799058340955526\n",
      "60 : -0.0775252436986193\n",
      "61 : 0.015075008152052762\n",
      "62 : -0.01550835508387536\n",
      "63 : 0.06331012714654206\n",
      "64 : -0.04432233117695431\n",
      "65 : 0.012035427085356785\n",
      "66 : 0.038518373481929304\n",
      "67 : -0.02661924185813405\n",
      "68 : 0.016876762412721284\n",
      "69 : 0.01369418785325252\n",
      "70 : -0.030677984585054222\n",
      "71 : -0.035439670050982384\n",
      "72 : 0.06390264974033925\n",
      "73 : -0.044228531629778446\n",
      "74 : 0.02021353041054681\n",
      "75 : 0.016391099826432765\n",
      "76 : 0.012559611178585327\n",
      "77 : 0.0007328931009396933\n",
      "78 : -0.07989887902513147\n",
      "79 : -0.051075889612548046\n",
      "80 : 0.047298558783950284\n",
      "81 : -0.015557367727160449\n",
      "82 : -0.09484114069491625\n",
      "83 : 0.012970924936234945\n",
      "84 : -0.09665777520276606\n",
      "85 : 0.1161326573230326\n",
      "86 : -0.16325007253326473\n",
      "87 : -0.08450818862766027\n",
      "88 : -0.14117588102817535\n",
      "89 : -0.10130329420790075\n",
      "90 : -0.08049552217125894\n",
      "91 : -0.1454031844390556\n",
      "92 : -0.020043857046402987\n",
      "93 : 0.034944074880331744\n",
      "94 : 0.025878923316486165\n",
      "95 : 0.031637431494891635\n",
      "96 : -0.06553469169884919\n",
      "97 : -0.09982865592464803\n",
      "98 : -0.02718499316833914\n",
      "99 : -0.07473449259996415\n",
      "0 : -0.18380753151141105\n",
      "1 : -0.06877377675846218\n",
      "2 : -0.05296317888423801\n",
      "3 : -0.0859216319397092\n",
      "4 : -0.08097241986542938\n",
      "5 : -0.008164718991611144\n",
      "6 : -0.14660253085894506\n",
      "7 : -0.050037077802699045\n",
      "8 : -0.058777688723057504\n",
      "9 : 0.02651512122247368\n",
      "10 : -0.03110310230404141\n",
      "11 : -0.19941788818687198\n",
      "12 : -0.11036010223906488\n",
      "13 : -0.12030510865151882\n",
      "14 : 0.032429253309965135\n",
      "15 : -0.05561659699305893\n",
      "16 : 0.004771512374281877\n",
      "17 : 0.01088332440704108\n",
      "18 : -0.09259616071358327\n",
      "19 : -0.10806228592991829\n",
      "20 : -0.09011180372908713\n",
      "21 : -0.04469159134896472\n",
      "22 : -0.15409449585713447\n",
      "23 : -0.09384781019762159\n",
      "24 : -0.024413190933410076\n",
      "25 : -0.20159009625203905\n",
      "26 : -0.039483014195866424\n",
      "27 : -0.09772869651205839\n",
      "28 : -0.02565388064831496\n",
      "29 : -0.042596851743292065\n",
      "30 : -0.09278635149821639\n",
      "31 : -0.0056365226744674105\n",
      "32 : -0.05112960916012525\n",
      "33 : -0.09131467230618001\n",
      "34 : 0.07110007442533968\n",
      "35 : -0.01459722695872188\n",
      "36 : -0.01833394530694932\n",
      "37 : 0.03784955188020831\n",
      "38 : -0.02876228883396834\n",
      "39 : -0.04116555333603174\n",
      "40 : 0.05139769734814763\n",
      "41 : -0.06285641139547805\n",
      "42 : -0.018053362499631475\n",
      "43 : 0.008256601192988452\n",
      "44 : 0.009967080666683614\n",
      "45 : -0.03361006094201002\n",
      "46 : -0.009212896891403942\n",
      "47 : -0.011024770673247988\n",
      "48 : -0.02501900572970043\n",
      "49 : -0.03420858387253248\n",
      "50 : 0.03475520081701688\n",
      "51 : -0.00872119177947752\n",
      "52 : -0.027205249445978553\n",
      "53 : -0.048627804545685646\n",
      "54 : -0.02311416387092322\n",
      "55 : -0.007080228801351041\n",
      "56 : 0.0277379835722968\n",
      "57 : -0.03760007459204643\n",
      "58 : 0.034562984574586154\n",
      "59 : -0.07446432379074396\n",
      "60 : -0.04008950470015407\n",
      "61 : -0.155258313100785\n",
      "62 : -0.001675052463542671\n",
      "63 : 0.04193589589558542\n",
      "64 : -0.04359052344225346\n",
      "65 : -0.14852177957072854\n",
      "66 : 0.01934446757659316\n",
      "67 : -0.028434878925327218\n",
      "68 : -0.033981239143759015\n",
      "69 : 0.08514453580137343\n",
      "70 : -0.04385830583050846\n",
      "71 : -0.10918301665224134\n",
      "72 : -0.0920825977344066\n",
      "73 : -0.07500889571383595\n",
      "74 : -0.10352471126243472\n",
      "75 : -0.09200864038430154\n",
      "76 : -0.1869478540495038\n",
      "77 : -0.12790534389205277\n",
      "78 : 0.03015617457567714\n",
      "79 : -0.07807909611146896\n",
      "80 : -0.08504606354981661\n",
      "81 : -0.0322918473975733\n",
      "82 : 0.08702323278412223\n",
      "83 : 0.00803440614836291\n",
      "84 : -0.045203476399183276\n",
      "85 : -0.057095672935247414\n",
      "86 : -0.1294242930598557\n",
      "87 : -0.10893353126011789\n",
      "88 : 0.01401514275348745\n",
      "89 : -0.07412263643927873\n",
      "90 : -0.05916634849272667\n",
      "91 : -0.07029016925953328\n",
      "92 : -0.05937042287550867\n",
      "93 : -0.029104073531925692\n",
      "94 : -0.1640511215198785\n",
      "95 : 0.04190392242744564\n",
      "96 : -0.039012424089014536\n",
      "97 : -0.17177426074631513\n",
      "98 : -0.10695867431786608\n",
      "99 : 0.007962820131797358\n",
      "0 : -0.0897549732020707\n",
      "1 : 0.027258061384782196\n",
      "2 : -0.11672025676816701\n",
      "3 : -0.060750295291654766\n",
      "4 : -0.10598608787986452\n",
      "5 : -0.032621568907052284\n",
      "6 : -0.04859612528234721\n",
      "7 : 0.0653578328434378\n",
      "8 : -0.083474160451442\n",
      "9 : -0.10192864155396825\n",
      "10 : -0.01853078535059466\n",
      "11 : -0.21868533813394606\n",
      "12 : -0.03226773677452001\n",
      "13 : -0.1287058061454445\n",
      "14 : -0.001027480838820343\n",
      "15 : -0.02923516780138016\n",
      "16 : -0.0633197202347219\n",
      "17 : -0.13522617258131503\n",
      "18 : -0.08538776393979787\n",
      "19 : -0.18012893218547105\n",
      "20 : -0.05773454448208213\n",
      "21 : -0.04716273059602827\n",
      "22 : -0.1175374482467305\n",
      "23 : -0.041654050792567426\n",
      "24 : -0.1118627321906388\n",
      "25 : -0.08175976779893972\n",
      "26 : -0.06885879235342145\n",
      "27 : -0.04714547517942264\n",
      "28 : -0.06065363129600883\n",
      "29 : -0.09349118715617806\n",
      "30 : -0.08509335480630398\n",
      "31 : -0.053987712529487926\n",
      "32 : -0.0925406026188284\n",
      "33 : -0.03283892162144186\n",
      "34 : -0.01872028568759561\n",
      "35 : -0.12512410320341588\n",
      "36 : -0.07212293073534967\n",
      "37 : -0.1601669017691165\n",
      "38 : -0.09726468948647381\n",
      "39 : 0.025832462171092627\n",
      "40 : -0.03989025376504287\n",
      "41 : -0.01409783600829541\n",
      "42 : -0.060197281138971444\n",
      "43 : -0.0357702009845525\n",
      "44 : -0.05238724351656854\n",
      "45 : -0.031792288614087734\n",
      "46 : -0.02991022916394286\n",
      "47 : -0.03139748316607438\n",
      "48 : -0.01684828888392076\n",
      "49 : -0.032749922986840825\n",
      "50 : -0.034248938178643586\n",
      "51 : -0.04026051827240735\n",
      "52 : 0.003972350992262369\n",
      "53 : -0.01230469421425369\n",
      "54 : -0.025416879961267114\n",
      "55 : -0.03814938098657876\n",
      "56 : -0.005389359418768435\n",
      "57 : -0.03915342426043935\n",
      "58 : 0.001999293998233042\n",
      "59 : -0.011804258928168568\n",
      "60 : -0.029957795621885455\n",
      "61 : -0.017721423145849256\n",
      "62 : 0.004002324800239879\n",
      "63 : -0.0015253692479745983\n",
      "64 : 0.01856847166636726\n",
      "65 : -0.018347435859323018\n",
      "66 : -0.0722945819958113\n",
      "67 : -0.03589710574597121\n",
      "68 : -0.03639882256102281\n",
      "69 : -0.051489976310404015\n",
      "70 : -0.021367538580670954\n",
      "71 : -0.02795480402128305\n",
      "72 : -0.003133854462066665\n",
      "73 : 0.008246254755795235\n",
      "74 : -0.01224437271885108\n",
      "75 : -0.0161536388204695\n",
      "76 : -0.00081814097466122\n",
      "77 : -0.01961579848721158\n",
      "78 : 0.015507280846941284\n",
      "79 : -0.0013298600169946435\n",
      "80 : -0.000986043449665886\n",
      "81 : -0.0005501037201611318\n",
      "82 : -0.02551294987206347\n",
      "83 : -0.008413106043008156\n",
      "84 : -0.03056707020441536\n",
      "85 : -0.028491903649410233\n",
      "86 : 0.00032082054531201726\n",
      "87 : -0.01873172567575238\n",
      "88 : -0.016010387835558505\n"
     ]
    }
   ],
   "source": [
    "curriculum = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "episodes = 20\n",
    "\n",
    "small_data = load_coco_data(max_train=50000)\n",
    "\n",
    "for level in curriculum:\n",
    "    \n",
    "    for epoch in range(100):        \n",
    "        episodicAvgLoss = 0\n",
    "        \n",
    "        captions, features, _ = sample_coco_minibatch(small_data, batch_size=episodes, split='train')\n",
    "        features = torch.tensor(features, device=device).float()\n",
    "        captions = torch.tensor(captions, device=device).long()\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            log_probs = []\n",
    "            values = []\n",
    "            rewards = []\n",
    "            caplen = np.nonzero(captions[episode] == 2)[0][0] + 1\n",
    "            \n",
    "            if (caplen - level > 1):\n",
    "                captions_in = captions[episode:episode+1, :caplen-level]\n",
    "                features_in = features[episode:episode+1]\n",
    "\n",
    "                for step in range(level):\n",
    "                    value, probs = a2cNetwork(features_in, captions_in)\n",
    "                    probs = F.softmax(probs, dim=2)\n",
    "                    \n",
    "                    dist = probs.cpu().detach().numpy()[0,0]\n",
    "                    action = np.random.choice(probs.shape[-1], p=dist)\n",
    "                    \n",
    "                    gen_cap = torch.from_numpy(np.array([action])).unsqueeze(0).to(device)\n",
    "                    captions_in = torch.cat((captions_in, gen_cap), axis=1)\n",
    "                    \n",
    "                    log_prob = torch.log(probs[0, 0, action])\n",
    "                    \n",
    "                    reward = GetRewards(features_in, captions_in, rewardNet)\n",
    "                    reward = reward.cpu().detach().numpy()[0, 0]\n",
    "                    \n",
    "                    rewards.append(reward)\n",
    "                    values.append(value)\n",
    "                    log_probs.append(log_prob)\n",
    "                    \n",
    "            values = torch.FloatTensor(values).to(device)\n",
    "            rewards = torch.FloatTensor(rewards).to(device)\n",
    "            log_probs = torch.stack(log_probs).to(device)\n",
    "            \n",
    "            advantage = values - rewards \n",
    "            actorLoss = (-log_probs * advantage).mean()\n",
    "            criticLoss = 0.5 * advantage.pow(2).mean()\n",
    "            \n",
    "            loss = actorLoss + criticLoss\n",
    "            episodicAvgLoss += loss.item()/episodes\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(epoch, \":\", episodicAvgLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "5375c6fca34c5f00efc2a1fb12328b4af153b74c7751f4a701371fb63dcb16d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
